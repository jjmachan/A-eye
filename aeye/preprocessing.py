# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_Preprocessing.ipynb (unless otherwise specified).

__all__ = ['Lang', 'SOS_token', 'EOS_token', 'indexesFromSentence', 'tensorFromSentence', 'tensorForImageCaption',
           'get_preprocessed_data', 'data_path', 'data_path']

# Cell
from pickle import dump
from pickle import load
from pathlib import Path

from torchvision import models
from torchvision import transforms
import torch
from torch import nn
from tqdm import tqdm



# Cell

SOS_token = 0
EOS_token = 1


class Lang:
    def __init__(self, name):
        self.name = name
        self.SOS_token = 0
        self.EOS_token = 1
        self.word2index = {}
        self.word2count = {}
        self.index2word = {0: "SOS", 1: "EOS"}
        self.n_words = 2  # Count SOS and EOS

#     def addSentence(self, sentence):
#         for word in sentence.split(' '):
#             self.addWord(word)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.n_words
            self.word2count[word] = 1
            self.index2word[self.n_words] = word
            self.n_words += 1
        else:
            self.word2count[word] += 1

# Cell

def indexesFromSentence(lang: Lang, sentence: list):
    return [lang.word2index[word] for word in sentence]


def tensorFromSentence(lang, sentence):
    device = torch.device('cpu' if torch.cuda.is_available else 'cpu')
    indexes = indexesFromSentence(lang, sentence)
    indexes.append(EOS_token)
    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)


def tensorForImageCaption(features_dict, sentence_tuple, lang):
    """
    This created the input_img, imput_vec pair that is used for training
    the model.

    Args:
        sentence_tuple: tuple with the img idx and tokenized sentence
        feature_dict: dict that maps the img idx and the feature tensor
        lang: The language model used.
    """
    idx, sentence = sentence_tuple
    target_tensor = tensorFromSentence(lang, sentence)
    features  = features_dict[idx]
    features = torch.from_numpy(features)
    return features, target_tensor

# Cell
data_path = '/home/jithin/datasets/imageCaptioning/flicker8k/preprocessed/'
data_path = Path(data_path)

def get_preprocessed_data(split: str):
    assert split in ['train', 'val', 'test']

    features_fname = 'flickr8k_features_%s'%(split)
    sentences_fname = 'sentence_list_%s_flickr8k'%(split)
    lang_fname = 'lang_%s_flickr8k'%(split)

    features = load(open(data_path/features_fname, 'rb'))
    sentence_list = load(open(data_path/sentences_fname, 'rb'))
    lang = load(open(data_path/lang_fname, 'rb'))

    return features, sentence_list,lang