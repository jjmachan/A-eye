{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.0 1.4.0\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "print(torchvision.__version__, torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr8k_dir = '/home/jithin/datasets/imageCaptioning/flicker8k/Flicker8k_Dataset'\n",
    "captions_file = '/home/jithin/datasets/imageCaptioning/captions/dataset_flickr8k.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['444881000_bba92e585c.jpg',\n",
       " '241345905_5826a72da1.jpg',\n",
       " '2318721455_80c6644441.jpg',\n",
       " '3365602213_dd3287a633.jpg',\n",
       " '539801139_7258ee437f.jpg',\n",
       " '3697378565_7060d9281a.jpg',\n",
       " '2147199188_d2d70b88ec.jpg',\n",
       " '143684568_3c59299bae.jpg',\n",
       " '1499495021_d295ce577c.jpg',\n",
       " '3081182021_22cfa18dd4.jpg']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(flickr8k_dir)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(captions_file, 'r') as f:\n",
    "    parsed_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json format\n",
    "\n",
    "There are 6k training data and 2k test data  \n",
    "  \n",
    "  \n",
    "file  \n",
    "|- images: list of all the sentences for each image  \n",
    "&nbsp;&nbsp;    |- sentids: list()  \n",
    "&nbsp;&nbsp;    |- imgid: int  \n",
    "&nbsp;&nbsp;    |- sentences: list()  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;        |- tokens: list(str) -> tokenized sentences  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;        |- raw: str  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;        |- imgid: int  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;        |- sentid: int  \n",
    "&nbsp;&nbsp;    |- split: ['train', 'val', 'test']  \n",
    "&nbsp;&nbsp;    |- filename: str -> only the filename of the image eg: 2513260012_03d33305cf.jpg  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentids': [34995, 34996, 34997, 34998, 34999],\n",
       " 'imgid': 6999,\n",
       " 'sentences': [{'tokens': ['a',\n",
       "    'girl',\n",
       "    'playing',\n",
       "    'is',\n",
       "    'a',\n",
       "    'pile',\n",
       "    'of',\n",
       "    'colorful',\n",
       "    'balls'],\n",
       "   'raw': 'A girl playing is a pile of colorful balls .',\n",
       "   'imgid': 6999,\n",
       "   'sentid': 34995},\n",
       "  {'tokens': ['a', 'little', 'girl', 'plays', 'in', 'a', 'ball', 'pit'],\n",
       "   'raw': 'A little girl plays in a ball pit .',\n",
       "   'imgid': 6999,\n",
       "   'sentid': 34996},\n",
       "  {'tokens': ['a',\n",
       "    'little',\n",
       "    'girl',\n",
       "    'plays',\n",
       "    'in',\n",
       "    'a',\n",
       "    'pit',\n",
       "    'of',\n",
       "    'colorful',\n",
       "    'balls'],\n",
       "   'raw': 'A little girl plays in a pit of colorful balls .',\n",
       "   'imgid': 6999,\n",
       "   'sentid': 34997},\n",
       "  {'tokens': ['a', 'small', 'girl', 'is', 'playing', 'in', 'a', 'ball', 'pit'],\n",
       "   'raw': 'A small girl is playing in a ball pit',\n",
       "   'imgid': 6999,\n",
       "   'sentid': 34998},\n",
       "  {'tokens': ['a',\n",
       "    'young',\n",
       "    'girl',\n",
       "    'with',\n",
       "    'a',\n",
       "    'white',\n",
       "    'shirt',\n",
       "    'and',\n",
       "    'pink',\n",
       "    'shorts',\n",
       "    'rolling',\n",
       "    'around',\n",
       "    'in',\n",
       "    'some',\n",
       "    'plastic',\n",
       "    'multicolored',\n",
       "    'balls'],\n",
       "   'raw': 'a young girl with a white shirt and pink shorts rolling around in some plastic multicolored balls',\n",
       "   'imgid': 6999,\n",
       "   'sentid': 34999}],\n",
       " 'split': 'val',\n",
       " 'filename': '522652105_a89f1cf260.jpg'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_json['images'][6999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "annotations = namedtuple('Annotations',['image_id','sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list()\n",
    "for image in parsed_json['images'][:6000]:\n",
    "    image_id = image['filename']\n",
    "    sentences_list = list()\n",
    "    for sentence in image['sentences']:\n",
    "        sentences_list.append(sentence['raw'])\n",
    "    train.append(annotations(image_id, sentences_list))\n",
    "    if image['split'] != 'train':\n",
    "        print(image)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Annotations(image_id='2513260012_03d33305cf.jpg', sentences=['A black dog is running after a white dog in the snow .', 'Black dog chasing brown dog through snow', 'Two dogs chase each other across the snowy ground .', 'Two dogs play together in the snow .', 'Two dogs running through a low lying body of water .'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "import json\n",
    "from collections import namedtuple\n",
    "\n",
    "annotations = namedtuple('Annotations',['image_id','sentences'])\n",
    "\n",
    "class Flickr8k(Dataset):\n",
    "    \"\"\" for flickr 8k dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, ann_file, split='train', transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): The root dir that points to the Flickr images.\n",
    "            ann_file (str): The file that contains the annotations for the images.\n",
    "            split ['train', 'val', 'test']: This decides which partition to load.\n",
    "            transform: Transforms for image.\n",
    "            target_transforms: transforms for sentences.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        assert split in ['train', 'test', 'val']\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.annotations = list()\n",
    "        \n",
    "        # indices when spliting the json file\n",
    "        if self.split == 'train':\n",
    "            m, n = 0, 6000\n",
    "        elif self.split == 'val':\n",
    "            m, n = 6000, 7000\n",
    "        elif self.split == 'test':\n",
    "            m, n = 7000, 8000\n",
    "            \n",
    "        with open(ann_file, 'r') as ann_file:\n",
    "            ann_json = json.load(ann_file)\n",
    "            for image in ann_json['images'][m : n]:\n",
    "                image_id = image['filename']\n",
    "                sentences_list = list()\n",
    "                for sentence in image['sentences']:\n",
    "                    sentences_list.append(sentence['raw'])\n",
    "                self.annotations.append(annotations(image_id, sentences_list))\n",
    "                \n",
    "                assert image['split'] == self.split\n",
    "                \n",
    "            print('loading %s complete'%(self.split))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_id = self.ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading val complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Flickr8k(flickr8k_dir, captions_file, split='val')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
