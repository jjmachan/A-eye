{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "\n",
    "> API details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append our Aeye package\n",
    "# TODO: fix this import issue when using the package.\n",
    "import sys \n",
    "sys.path.append('../Aeye')\n",
    "\n",
    "from datasets import Flickr8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from pathlib import Path\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.SOS_token = 0\n",
    "        self.EOS_token = 1\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "#     def addSentence(self, sentence):\n",
    "#         for word in sentence.split(' '):\n",
    "#             self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train complete\n",
      "loading val complete\n",
      "loading test complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flickr8k_img_dir = '/home/jithin/datasets/imageCaptioning/flicker8k/Flicker8k_Dataset'\n",
    "annotation_file = '/home/jithin/datasets/imageCaptioning/captions/dataset_flickr8k.json'\n",
    "\n",
    "transform = transforms.Compose([            \n",
    " transforms.Resize(256),                    \n",
    " transforms.CenterCrop(224),        \n",
    " transforms.ToTensor(),                     \n",
    " transforms.Normalize(                      \n",
    " mean=[0.485, 0.456, 0.406],                \n",
    " std=[0.229, 0.224, 0.225]                  \n",
    " )])\n",
    "\n",
    "train = Flickr8k(flickr8k_img_dir, annotation_file, split=\"train\", transform=transform)\n",
    "val = Flickr8k(flickr8k_img_dir, annotation_file, split=\"val\", transform=transform)\n",
    "test = Flickr8k(flickr8k_img_dir, annotation_file, split=\"test\", transform=transform)\n",
    "\n",
    "datasets = [train, val, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 3\n",
    "dataset_train[item][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train[item][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train[item][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_encodings(model, dataset):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Force to cpu due to memory issue in gpu\n",
    "    # TODO: fix memory issue in gpu\n",
    "    #device = torch.device('cpu')\n",
    "    features = dict()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for img_id, img, sentences in tqdm(dataset):\n",
    "            img = torch.unsqueeze(img, 0).to(device)\n",
    "            feature = model(img)\n",
    "            features[img_id] = feature.cpu().numpy()\n",
    "            del img,feature\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = Identity()\n",
    "\n",
    "print('The Model Architecture:')\n",
    "print('------------------------')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_names = ['train', 'val', 'test']\n",
    "file_name = 'flickr8k_features_'\n",
    "out_dir = Path('/home/jithin/datasets/imageCaptioning/flicker8k/preprocessed/')\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    features = get_img_encodings(model, dataset)\n",
    "    print('Saving in %s%s'%(file_name,split_names[i]))\n",
    "    dump(features, open(out_dir/(file_name+split_names[i]), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_lists(dataset, language_name):\n",
    "    lang = Lang(language_name)\n",
    "    sentences_list = list()\n",
    "\n",
    "    for idx, _, sentences in tqdm(dataset):\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                lang.addWord(word)\n",
    "\n",
    "            sentences_list.append((idx, sentence))\n",
    "    return sentences_list,lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_list, lang = get_sentence_lists(datasets[2], 'flickr8k')\n",
    "\n",
    "# print(lang.n_words, len(sentence_list))\n",
    "\n",
    "# dump(lang, open(out_dir/'lang_val_flickr8k', 'wb'))\n",
    "# dump(sentence_list, open(out_dir/'sentence_list_val_flickr8k', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def indexesFromSentence(lang: Lang, sentence: list):\n",
    "    return [lang.word2index[word] for word in sentence]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    device = torch.device('cpu' if torch.cuda.is_available else 'cpu')\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorForImageCaption(features_dict, sentence_tuple, lang):\n",
    "    \"\"\"\n",
    "    This created the input_img, imput_vec pair that is used for training\n",
    "    the model.\n",
    "    \n",
    "    Args:\n",
    "        sentence_tuple: tuple with the img idx and tokenized sentence\n",
    "        feature_dict: dict that maps the img idx and the feature tensor\n",
    "        lang: The language model used.\n",
    "    \"\"\"\n",
    "    idx, sentence = sentence_tuple\n",
    "    target_tensor = tensorFromSentence(lang, sentence)\n",
    "    features  = features_dict[idx]\n",
    "    features = torch.from_numpy(features)\n",
    "    return features, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "img, sentence = tensorImageCaption(features, lang, sentence_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "data_path = '/home/jithin/datasets/imageCaptioning/flicker8k/preprocessed/'\n",
    "data_path = Path(data_path)\n",
    "\n",
    "def get_preprocessed_data(split: str):\n",
    "    assert split in ['train', 'val', 'test']\n",
    "    \n",
    "    features_fname = 'flickr8k_features_%s'%(split)\n",
    "    sentences_fname = 'sentence_list_%s_flickr8k'%(split)\n",
    "    lang_fname = 'lang_%s_flickr8k'%(split)\n",
    "    \n",
    "    features = load(open(data_path/features_fname, 'rb'))\n",
    "    sentence_list = load(open(data_path/sentences_fname, 'rb'))\n",
    "    lang = load(open(data_path/lang_fname, 'rb'))\n",
    "    \n",
    "    return features, sentence_list,lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2fef94666eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preprocessed_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e6e0be89373d>\u001b[0m in \u001b[0;36mget_preprocessed_data\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_preprocessed_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfeatures_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'flickr8k_features_%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_, sentences, lang = get_preprocessed_data('tes')\n",
    "lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
