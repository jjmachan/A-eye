{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.0 1.4.0\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "print(torchvision.__version__, torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr8k_dir = Path('/home/jithin/datasets/imageCaptioning/flicker8k/Flicker8k_Dataset')\n",
    "captions_file = Path('/home/jithin/datasets/imageCaptioning/captions/dataset_flickr8k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['444881000_bba92e585c.jpg',\n",
       " '241345905_5826a72da1.jpg',\n",
       " '2318721455_80c6644441.jpg',\n",
       " '3365602213_dd3287a633.jpg',\n",
       " '539801139_7258ee437f.jpg',\n",
       " '3697378565_7060d9281a.jpg',\n",
       " '2147199188_d2d70b88ec.jpg',\n",
       " '143684568_3c59299bae.jpg',\n",
       " '1499495021_d295ce577c.jpg',\n",
       " '3081182021_22cfa18dd4.jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir(flickr8k_dir)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(captions_file, 'r') as f:\n",
    "    parsed_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json format\n",
    "\n",
    "There are 6k training data and 2k test data  \n",
    "  \n",
    "  \n",
    "file  \n",
    "|- images: list of all the sentences for each image  \n",
    "&nbsp;&nbsp;    |- sentids: list()  \n",
    "&nbsp;&nbsp;    |- imgid: int  \n",
    "&nbsp;&nbsp;    |- sentences: list()  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;        |- tokens: list(str) -> tokenized sentences  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;        |- raw: str  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;        |- imgid: int  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;        |- sentid: int  \n",
    "&nbsp;&nbsp;    |- split: ['train', 'val', 'test']  \n",
    "&nbsp;&nbsp;    |- filename: str -> only the filename of the image eg: 2513260012_03d33305cf.jpg  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentids': [34995, 34996, 34997, 34998, 34999],\n",
       " 'imgid': 6999,\n",
       " 'sentences': [{'tokens': ['a',\n",
       "    'girl',\n",
       "    'playing',\n",
       "    'is',\n",
       "    'a',\n",
       "    'pile',\n",
       "    'of',\n",
       "    'colorful',\n",
       "    'balls'],\n",
       "   'raw': 'A girl playing is a pile of colorful balls .',\n",
       "   'imgid': 6999,\n",
       "   'sentid': 34995},\n",
       "  {'tokens': ['a', 'little', 'girl', 'plays', 'in', 'a', 'ball', 'pit'],\n",
       "   'raw': 'A little girl plays in a ball pit .',\n",
       "   'imgid': 6999,\n",
       "   'sentid': 34996},\n",
       "  {'tokens': ['a',\n",
       "    'little',\n",
       "    'girl',\n",
       "    'plays',\n",
       "    'in',\n",
       "    'a',\n",
       "    'pit',\n",
       "    'of',\n",
       "    'colorful',\n",
       "    'balls'],\n",
       "   'raw': 'A little girl plays in a pit of colorful balls .',\n",
       "   'imgid': 6999,\n",
       "   'sentid': 34997},\n",
       "  {'tokens': ['a', 'small', 'girl', 'is', 'playing', 'in', 'a', 'ball', 'pit'],\n",
       "   'raw': 'A small girl is playing in a ball pit',\n",
       "   'imgid': 6999,\n",
       "   'sentid': 34998},\n",
       "  {'tokens': ['a',\n",
       "    'young',\n",
       "    'girl',\n",
       "    'with',\n",
       "    'a',\n",
       "    'white',\n",
       "    'shirt',\n",
       "    'and',\n",
       "    'pink',\n",
       "    'shorts',\n",
       "    'rolling',\n",
       "    'around',\n",
       "    'in',\n",
       "    'some',\n",
       "    'plastic',\n",
       "    'multicolored',\n",
       "    'balls'],\n",
       "   'raw': 'a young girl with a white shirt and pink shorts rolling around in some plastic multicolored balls',\n",
       "   'imgid': 6999,\n",
       "   'sentid': 34999}],\n",
       " 'split': 'val',\n",
       " 'filename': '522652105_a89f1cf260.jpg'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_json['images'][6999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "annotations = namedtuple('Annotations',['image_id','sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list()\n",
    "for image in parsed_json['images'][:6000]:\n",
    "    image_id = image['filename']\n",
    "    sentences_list = list()\n",
    "    for sentence in image['sentences']:\n",
    "        sentences_list.append(sentence['tokens'])\n",
    "    train.append(annotations(image_id, sentences_list))\n",
    "    if image['split'] != 'train':\n",
    "        print(image)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Annotations(image_id='2513260012_03d33305cf.jpg', sentences=[['a', 'black', 'dog', 'is', 'running', 'after', 'a', 'white', 'dog', 'in', 'the', 'snow'], ['black', 'dog', 'chasing', 'brown', 'dog', 'through', 'snow'], ['two', 'dogs', 'chase', 'each', 'other', 'across', 'the', 'snowy', 'ground'], ['two', 'dogs', 'play', 'together', 'in', 'the', 'snow'], ['two', 'dogs', 'running', 'through', 'a', 'low', 'lying', 'body', 'of', 'water']])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "import json\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "annotations = namedtuple('Annotations',['image_id','sentences'])\n",
    "\n",
    "class Flickr8k(Dataset):\n",
    "    \"\"\" for flickr 8k dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_dir, ann_file, split='train', transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): The root dir that points to the Flickr images.\n",
    "            ann_file (str): The file that contains the annotations for the images.\n",
    "            split ['train', 'val', 'test']: This decides which partition to load.\n",
    "            transform: Transforms for image.\n",
    "            target_transforms: transforms for sentences.\n",
    "        \"\"\"\n",
    "        self.img_dir = Path(img_dir)\n",
    "        assert split in ['train', 'test', 'val']\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.annotations = list()\n",
    "        \n",
    "        # indices when spliting the json file\n",
    "        if self.split == 'train':\n",
    "            m, n = 0, 6000\n",
    "        elif self.split == 'val':\n",
    "            m, n = 6000, 7000\n",
    "        elif self.split == 'test':\n",
    "            m, n = 7000, 8000\n",
    "            \n",
    "        with open(ann_file, 'r') as ann_file:\n",
    "            ann_json = json.load(ann_file)\n",
    "            for image in ann_json['images'][m : n]:\n",
    "                image_id = image['filename']\n",
    "                sentences_list = list()\n",
    "                for sentence in image['sentences']:\n",
    "                    sentences_list.append(sentence['tokens'])\n",
    "                self.annotations.append(annotations(image_id, sentences_list))\n",
    "                \n",
    "                assert image['split'] == self.split\n",
    "                \n",
    "            print('loading %s complete'%(self.split))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.annotations[index].image_id\n",
    "        \n",
    "        img = Image.open(self.img_dir/img_id).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        # Captions\n",
    "        target = self.annotations[index].sentences\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading val complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Flickr8k(flickr8k_dir, captions_file, split='val')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=500x333 at 0x7F7E8E42ECC0>,\n",
       " [['the',\n",
       "   'boy',\n",
       "   'laying',\n",
       "   'face',\n",
       "   'down',\n",
       "   'on',\n",
       "   'a',\n",
       "   'skateboard',\n",
       "   'is',\n",
       "   'being',\n",
       "   'pushed',\n",
       "   'along',\n",
       "   'the',\n",
       "   'ground',\n",
       "   'by',\n",
       "   'another',\n",
       "   'boy'],\n",
       "  ['two', 'girls', 'play', 'on', 'a', 'skateboard', 'in', 'a', 'courtyard'],\n",
       "  ['two', 'people', 'play', 'on', 'a', 'long', 'skateboard'],\n",
       "  ['two',\n",
       "   'small',\n",
       "   'children',\n",
       "   'in',\n",
       "   'red',\n",
       "   'shirts',\n",
       "   'playing',\n",
       "   'on',\n",
       "   'a',\n",
       "   'skateboard'],\n",
       "  ['two',\n",
       "   'young',\n",
       "   'children',\n",
       "   'on',\n",
       "   'a',\n",
       "   'skateboard',\n",
       "   'going',\n",
       "   'across',\n",
       "   'a',\n",
       "   'sidewalk']])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Annotations(image_id='2090545563_a4e66ec76b.jpg', sentences=[['the', 'boy', 'laying', 'face', 'down', 'on', 'a', 'skateboard', 'is', 'being', 'pushed', 'along', 'the', 'ground', 'by', 'another', 'boy'], ['two', 'girls', 'play', 'on', 'a', 'skateboard', 'in', 'a', 'courtyard'], ['two', 'people', 'play', 'on', 'a', 'long', 'skateboard'], ['two', 'small', 'children', 'in', 'red', 'shirts', 'playing', 'on', 'a', 'skateboard'], ['two', 'young', 'children', 'on', 'a', 'skateboard', 'going', 'across', 'a', 'sidewalk']])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
